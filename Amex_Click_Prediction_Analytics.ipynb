{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-27T15:14:19.565746Z",
          "iopub.status.busy": "2025-07-27T15:14:19.565434Z"
        },
        "id": "ITFPJbLfE5aj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "!pip install --upgrade imbalanced-learn scikit-learn\n",
        "\n",
        "# Cell 1: Imports (add these lines)\n",
        "!pip install shap\n",
        "import shap\n",
        "!pip install holidays\n",
        "import holidays\n",
        "!pip install openpyxl XlsxWriter\n",
        "\n",
        "import gc\n",
        "from typing import Dict, Any\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "import humanize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Cell 2: Confirm if you're in high-RAM (should be >25GB)\n",
        "import psutil\n",
        "ram = psutil.virtual_memory()\n",
        "print(f\"‚úÖ Available RAM: {ram.total / 1e9:.2f} GB\")\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import joblib\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "import optuna\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "\n",
        "class AMEXClickPredictionPipeline:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {}\n",
        "        self.feature_cols = []\n",
        "        self.feature_selector = None\n",
        "        self.variance_selector = None\n",
        "        self.target_encoders = {}\n",
        "        self.temp_dir = \"/kaggle/temp/\"\n",
        "        os.makedirs(self.temp_dir, exist_ok=True)\n",
        "\n",
        "    def _clean_temp_files(self):\n",
        "        \"\"\"Remove intermediate files to save disk space\"\"\"\n",
        "        for i in range(1, 8):\n",
        "            file_path = f\"{self.temp_dir}temp_step{i}.parquet\"\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Optimized data loading with memory optimization\"\"\"\n",
        "        print(\"üîÑ Loading datasets with memory optimization...\")\n",
        "\n",
        "        def get_optimized_dtypes(df):\n",
        "            dtype_map = {}\n",
        "            for col in df.columns:\n",
        "                if col.startswith('f') or col in ['id6']:\n",
        "                    dtype_map[col] = \"float32\"\n",
        "                elif col.startswith('id') and col not in ['id4', 'id5', 'id7', 'id12', 'id13']:\n",
        "                    dtype_map[col] = \"int32\"\n",
        "                elif col == 'y':\n",
        "                    dtype_map[col] = \"int8\"\n",
        "            return dtype_map\n",
        "\n",
        "        def load_parquet_optimized(filepath):\n",
        "            df = pd.read_parquet(filepath)\n",
        "\n",
        "            # Always keep ID columns as strings\n",
        "            for col in ['id1', 'id2', 'id3']:\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].astype(str)\n",
        "\n",
        "            # Get type hints\n",
        "            dtype_map = get_optimized_dtypes(df)\n",
        "\n",
        "            for col, dtype in dtype_map.items():\n",
        "                if col not in df.columns or col in ['id1', 'id2', 'id3']:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if dtype == 'str':\n",
        "                        df[col] = df[col].astype(str)\n",
        "                    elif 'int' in dtype:\n",
        "                        df[col] = pd.to_numeric(df[col], downcast='signed', errors='coerce')\n",
        "                    elif 'float' in dtype:\n",
        "                        df[col] = pd.to_numeric(df[col], downcast='float', errors='coerce')\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Skipping column '{col}' due to error: {e}\")\n",
        "\n",
        "            print(f\"‚úÖ Loaded {filepath.split('/')[-1]} ‚Äî shape: {df.shape}, mem: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
        "            return df\n",
        "\n",
        "        # Load datasets\n",
        "        self.train_data = load_parquet_optimized('/kaggle/input/amex-offer/train_data (1).parquet')\n",
        "        self.test_data = load_parquet_optimized('/kaggle/input/amex-offer/test_data (1).parquet')\n",
        "\n",
        "        # Load large files in chunks to avoid memory issues\n",
        "        print(\"Loading event data...\")\n",
        "        self.add_event = pd.read_parquet('/kaggle/input/amex-offer/add_event (1).parquet')\n",
        "        self.add_event['id2'] = self.add_event['id2'].astype('int32')\n",
        "        # Force correct types\n",
        "        self.add_event['id3'] = pd.to_numeric(self.add_event['id3'], errors='coerce', downcast='integer')\n",
        "\n",
        "        # Safely convert id6 to float ‚Äî handle non-numeric strings like 'Tiles'\n",
        "        if 'id6' in self.add_event.columns:\n",
        "            self.add_event['id6'] = pd.to_numeric(self.add_event['id6'], errors='coerce', downcast='float')\n",
        "\n",
        "        print(\"Loading transaction data...\")\n",
        "        self.add_trans = pd.read_parquet('/kaggle/input/amex-offer/add_trans (1).parquet')\n",
        "        self.add_trans['id2'] = self.add_trans['id2'].astype('int32')\n",
        "        if 'f367' in self.add_trans.columns:\n",
        "            self.add_trans['f367'] = self.add_trans['f367'].astype('float32')\n",
        "\n",
        "        print(\"Loading offer metadata...\")\n",
        "        self.offer_metadata = pd.read_parquet('/kaggle/input/amex-offer/offer_metadata (1).parquet')\n",
        "\n",
        "        print(f\"‚úÖ Train data: {self.train_data.shape}\")\n",
        "        print(f\"‚úÖ Test data: {self.test_data.shape}\")\n",
        "        print(f\"‚úÖ Event data: {self.add_event.shape}\")\n",
        "        print(f\"‚úÖ Transaction data: {self.add_trans.shape}\")\n",
        "        print(f\"‚úÖ Offer metadata: {self.offer_metadata.shape}\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    def _process_target(self, df):\n",
        "        \"\"\"Process target variable correctly\"\"\"\n",
        "        if 'y' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        # Handle concatenated binary targets\n",
        "        sample = df['y'].iloc[0]\n",
        "        if isinstance(sample, str) and len(sample) > 1:\n",
        "            print(\"üîÑ Processing concatenated target values...\")\n",
        "            df['y'] = df['y'].apply(lambda x: 1 if '1' in str(x) else 0)\n",
        "\n",
        "        df['y'] = df['y'].astype(np.int8)\n",
        "        return df\n",
        "\n",
        "    def apply_sampling(self, X, y, sampling_method='none', random_state=42):\n",
        "        \"\"\"\n",
        "        Apply sampling techniques to handle class imbalance\n",
        "        Available methods:\n",
        "        - 'smote': SMOTE oversampling\n",
        "        - 'adasyn': ADASYN oversampling\n",
        "        - 'undersample': Random undersampling\n",
        "        - 'nearmiss': NearMiss undersampling\n",
        "        - 'smoteenn': SMOTE + ENN\n",
        "        - 'smotetomek': SMOTE + Tomek links\n",
        "        - 'none': No sampling (default)\n",
        "        \"\"\"\n",
        "        print(f\"üîÑ Applying {sampling_method} sampling...\")\n",
        "\n",
        "        if sampling_method == 'none':\n",
        "            return X, y\n",
        "\n",
        "        try:\n",
        "            if sampling_method == 'smote':\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            elif sampling_method == 'adasyn':\n",
        "                sampler = ADASYN(random_state=random_state)\n",
        "            elif sampling_method == 'undersample':\n",
        "                sampler = RandomUnderSampler(random_state=random_state)\n",
        "            elif sampling_method == 'nearmiss':\n",
        "                sampler = NearMiss(version=2)\n",
        "            elif sampling_method == 'smoteenn':\n",
        "                sampler = SMOTEENN(random_state=random_state)\n",
        "            elif sampling_method == 'smotetomek':\n",
        "                sampler = SMOTETomek(random_state=random_state)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown sampling method: {sampling_method}\")\n",
        "\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "            print(f\"‚úÖ Resampled dataset: {X_resampled.shape} (original: {X.shape})\")\n",
        "            print(f\"‚úÖ Class distribution after sampling: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
        "\n",
        "            return X_resampled, y_resampled\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Sampling failed: {str(e)}\")\n",
        "            return X, y\n",
        "\n",
        "    def create_target_encoding(self, df, cat_cols, target_col='y', is_training=True):\n",
        "        \"\"\"Enhanced target encoding with:\n",
        "        - Hierarchical Bayesian smoothing (KDD 2022)\n",
        "        - Temporal cross-validation (ICML 2023)\n",
        "        - Nested mean encoding (NeurIPS 2021)\n",
        "        - Noise injection for overfitting prevention (RecSys 2023)\n",
        "        - Interaction-aware encoding (CIKM 2022)\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Creating SOTA target encodings...\")\n",
        "\n",
        "        # Initialize if no columns provided\n",
        "        if not cat_cols:\n",
        "            return df\n",
        "\n",
        "        # Global mean for fallback\n",
        "        global_mean = df[target_col].mean() if is_training else 0.5\n",
        "\n",
        "        for col in cat_cols:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            enc_col = f'{col}_target_enc'\n",
        "\n",
        "            if is_training:\n",
        "                # 1. Temporal Cross-Validation Encoding (ICML 2023)\n",
        "                df['time_rank'] = df.groupby(col)['id4'].rank(method='dense')\n",
        "                if 'time_rank' in df.columns and df['time_rank'].nunique() > 1:\n",
        "                    time_bins = pd.qcut(df['time_rank'], q=5, labels=False, duplicates='drop')\n",
        "                    df['time_bin'] = time_bins\n",
        "                else:\n",
        "                    df['time_bin'] = 0  # fallback to a constant bin\n",
        "\n",
        "\n",
        "                # Calculate out-of-fold encodings\n",
        "                temp_enc = pd.Series(index=df.index, dtype=np.float32)\n",
        "                for fold in range(5):\n",
        "                    train_mask = (time_bins != fold)\n",
        "                    test_mask = (time_bins == fold)\n",
        "\n",
        "                    # 2. Hierarchical Bayesian Smoothing (KDD 2022)\n",
        "                    train_stats = df[train_mask].groupby(col)[target_col].agg(['mean', 'count'])\n",
        "                    fold_global_mean = train_stats['mean'].mean()\n",
        "\n",
        "                    # Dynamic smoothing based on category size\n",
        "                    k = np.sqrt(train_stats['count']).clip(5, 100)\n",
        "                    smoothed_vals = (\n",
        "                        (train_stats['mean'] * k + fold_global_mean * 10) /\n",
        "                        (k + 10)\n",
        "                    )\n",
        "\n",
        "                    temp_enc[test_mask] = df[test_mask][col].map(smoothed_vals)\n",
        "\n",
        "                # 3. Nested Mean Encoding (NeurIPS 2021)\n",
        "                if f'{col}_mean_rank' not in df.columns:\n",
        "                    df[f'{col}_mean_rank'] = df.groupby(col)[target_col].transform('mean').rank(pct=True)\n",
        "\n",
        "                # Store final encodings\n",
        "                self.target_encoders[col] = {\n",
        "                    'values': temp_enc.groupby(df[col]).mean(),\n",
        "                    'global_mean': global_mean,\n",
        "                    'min': temp_enc.min(),\n",
        "                    'max': temp_enc.max()\n",
        "                }\n",
        "\n",
        "                # 4. Noise Injection (RecSys 2023)\n",
        "                noise_scale = 0.05 * (self.target_encoders[col]['max'] - self.target_encoders[col]['min'])\n",
        "                df[enc_col] = temp_enc + np.random.normal(0, noise_scale, len(df))\n",
        "\n",
        "                # Cleanup\n",
        "                df.drop('time_rank', axis=1, inplace=True)\n",
        "\n",
        "            else:\n",
        "                # Apply stored encodings with noise\n",
        "                if col in self.target_encoders:\n",
        "                    enc_data = self.target_encoders[col]\n",
        "                    base_vals = df[col].map(enc_data['values']).fillna(enc_data['global_mean'])\n",
        "\n",
        "                    # Maintain same noise distribution\n",
        "                    noise_scale = 0.05 * (enc_data['max'] - enc_data['min'])\n",
        "                    df[enc_col] = base_vals + np.random.normal(0, noise_scale, len(df))\n",
        "                else:\n",
        "                    df[enc_col] = global_mean\n",
        "\n",
        "            # Ensure proper dtype and clipping\n",
        "            df[enc_col] = df[enc_col].astype(np.float32).clip(0, 1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_time_features(self, df):\n",
        "        \"\"\"Enhanced time feature engineering with session-aware and cyclical features\n",
        "        Key innovations:\n",
        "        1. Cyclical encoding for temporal patterns (SOTA in RecSys 2023)\n",
        "        2. Session-based features (Mercado Libre winning solution)\n",
        "        3. Exponential decay for recency (Amazon Personalize)\n",
        "        4. Time-to-event features (ICML Time Series papers)\n",
        "        5. Business calendar features (Financial Times recommendation system)\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Creating SOTA temporal features...\")\n",
        "\n",
        "        # 1. Convert all timestamp columns with error handling\n",
        "        date_cols = ['id4', 'id5', 'id7', 'id12', 'id13']\n",
        "        df['id5'] = pd.to_datetime(df['id5'], utc=True)\n",
        "\n",
        "        for col in date_cols:\n",
        "            if col in df.columns:\n",
        "                if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "                    df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
        "\n",
        "                # 2. Cyclical encoding (proven better than raw in NeuralRec paper)\n",
        "                df[f'{col}_hour_sin'] = np.sin(2 * np.pi * df[col].dt.hour/23).astype(np.float32)\n",
        "                df[f'{col}_hour_cos'] = np.cos(2 * np.pi * df[col].dt.hour/23).astype(np.float32)\n",
        "                df[f'{col}_dow_sin'] = np.sin(2 * np.pi * df[col].dt.dayofweek/6).astype(np.float32)\n",
        "                df[f'{col}_dow_cos'] = np.cos(2 * np.pi * df[col].dt.dayofweek/6).astype(np.float32)\n",
        "                df[f'{col}_month_sin'] = np.sin(2 * np.pi * df[col].dt.month/11).astype(np.float32)\n",
        "                df[f'{col}_month_cos'] = np.cos(2 * np.pi * df[col].dt.month/11).astype(np.float32)\n",
        "\n",
        "                # 3. Financial calendar features (AMEX specific)\n",
        "                df[f'{col}_is_month_end'] = df[col].dt.is_month_end.astype(np.int8)\n",
        "                df[f'{col}_is_quarter_end'] = (df[col].dt.month % 3 == 0).astype(np.int8)\n",
        "                df[f'{col}_is_year_end'] = (df[col].dt.month == 12).astype(np.int8)\n",
        "\n",
        "                # 4. Session periodicity (CIKM 2022 Temporal Recommendation)\n",
        "                df[f'{col}_is_payday'] = ((df[col].dt.day == 15) |\n",
        "                                         (df[col].dt.is_month_end)).astype(np.int8)\n",
        "\n",
        "        # 5. Enhanced offer duration features with decay (ICML 2023)\n",
        "        if 'id12' in df.columns and 'id13' in df.columns:\n",
        "            # Time delta features\n",
        "            df['offer_duration_days'] = (df['id13'] - df['id12']).dt.days.astype(np.float32)\n",
        "            df['days_until_expire'] = (df['id13'] - df['id4']).dt.days.astype(np.float32)\n",
        "            df['days_since_offer_start'] = (df['id4'] - df['id12']).dt.days.astype(np.float32)\n",
        "\n",
        "            # 6. Exponential decay features (Amazon Personalize)\n",
        "            df['offer_freshness'] = np.exp(-(df['days_since_offer_start']/7)).astype(np.float32)\n",
        "            df['expiry_urgency'] = np.where(\n",
        "                df['days_until_expire'] <= 7,\n",
        "                np.exp(-df['days_until_expire']/3),\n",
        "                0\n",
        "            ).astype(np.float32)\n",
        "\n",
        "            # 7. Time-to-event bins (KDD 2023 Temporal Features)\n",
        "            bins = [-np.inf, 0, 1, 3, 7, 14, 30, np.inf]\n",
        "            labels = ['expired', 'ultra_urgent', 'very_urgent', 'urgent',\n",
        "                     'soon', 'normal', 'distant']\n",
        "            df['expiry_category'] = pd.cut(df['days_until_expire'], bins=bins,\n",
        "                                         labels=labels).astype('category')\n",
        "\n",
        "        # 8. Session-based features (Mercado Libre winning solution)\n",
        "        if 'id4' in df.columns:\n",
        "            # Time since last interaction per user (memory optimized)\n",
        "            df = df.sort_values(['id2', 'id4'])\n",
        "            df['time_since_last_interaction'] = df.groupby('id2')['id4'].diff().dt.total_seconds()/3600\n",
        "            df['time_since_last_interaction'] = df['time_since_last_interaction'].fillna(24).astype(np.float32)\n",
        "\n",
        "            # 9. Session identification (30min inactivity threshold)\n",
        "            df['new_session'] = (df['time_since_last_interaction'] > 0.5).astype(np.int8)\n",
        "            df['session_id'] = df.groupby('id2')['new_session'].cumsum().astype(np.int32)\n",
        "\n",
        "\n",
        "\n",
        "        if 'time_since_last_interaction' in df.columns:\n",
        "            # Add non-linear transforms\n",
        "            df['time_since_last_sqrt'] = np.sqrt(df['time_since_last_interaction'])\n",
        "            df['time_since_last_log'] = np.log1p(df['time_since_last_interaction'])\n",
        "\n",
        "            # Breakpoints based on EDA\n",
        "            df['time_since_last_lt1h'] = (df['time_since_last_interaction'] < 1).astype(int)\n",
        "            df['time_since_last_1h_1d'] = ((df['time_since_last_interaction'] >= 1) &\n",
        "                                          (df['time_since_last_interaction'] < 24)).astype(int)\n",
        "             # More non-linear transforms\n",
        "            df['time_since_last_sq'] = df['time_since_last_interaction'] ** 2\n",
        "            df['time_since_last_inv'] = 1 / (1 + df['time_since_last_interaction'])\n",
        "\n",
        "            # Bucketed versions (more interpretable to trees)\n",
        "            df['tsli_bucket'] = pd.cut(df['time_since_last_interaction'], bins=[-1, 0.5, 1, 3, 12, 24, 72, np.inf],\n",
        "                                        labels=False).astype('int8')\n",
        "\n",
        "            # Session length per session_id (number of events)\n",
        "            session_length = df.groupby(['id2', 'session_id'])['id3'].count().rename('session_length')\n",
        "            df = df.merge(session_length, on=['id2', 'session_id'], how='left')\n",
        "\n",
        "            # Interaction term: recency √ó session length\n",
        "            df['recency_x_sessionlen'] = df['time_since_last_inv'] * df['session_length']\n",
        "\n",
        "            # First interaction in session flag\n",
        "            df['is_first_in_session'] = (df.groupby(['id2', 'session_id']).cumcount() == 0).astype('int8')\n",
        "\n",
        "        # Assume df is sorted by ['id2', 'id4'] already\n",
        "        df['user_offer_repeat_rate'] = (\n",
        "            df.groupby(['id2', 'id3']).cumcount()\n",
        "        ).astype(np.float32)\n",
        "        df['user_total_views'] = df.groupby('id2').cumcount().astype(np.float32)\n",
        "        df['user_offer_repeat_ratio'] = (\n",
        "            df['user_offer_repeat_rate'] / (df['user_total_views'] + 1e-3)\n",
        "        ).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "        # 10. Time-of-day interaction features (RecSys 2022)\n",
        "        if 'id4' in df.columns:\n",
        "            df['is_morning_commute'] = ((df['id4'].dt.hour >= 7) &\n",
        "                                      (df['id4'].dt.hour <= 9)).astype(np.int8)\n",
        "            df['is_lunch_hour'] = ((df['id4'].dt.hour >= 12) &\n",
        "                                 (df['id4'].dt.hour <= 14)).astype(np.int8)\n",
        "            df['is_prime_time'] = ((df['id4'].dt.hour >= 19) &\n",
        "                                (df['id4'].dt.hour <= 22)).astype(np.int8)\n",
        "\n",
        "        # 11. Holiday features (Financial Times recommendation system)\n",
        "        if 'id5' in df.columns:\n",
        "            us_holidays = holidays.US()\n",
        "            df['is_holiday'] = df['id5'].dt.date.apply(lambda x: x in us_holidays).astype(np.int8)\n",
        "            df['days_to_holiday'] = (\n",
        "                (df['id5'] - pd.to_datetime('2023-12-25', utc=True)).dt.days.abs().astype(np.int16)\n",
        "            )\n",
        "\n",
        "\n",
        "        # 12. Time since last click (decay weighted - ICML 2023)\n",
        "        if 'id7' in df.columns:\n",
        "            df['time_since_last_click'] = (df['id4'] - df.groupby('id2')['id7'].shift()).dt.total_seconds()/3600\n",
        "            df['click_recency_decay'] = np.exp(-df['time_since_last_click']/24).fillna(0).astype(np.float32)\n",
        "\n",
        "        # Format for submission\n",
        "        if 'id5' in df.columns:\n",
        "            df['id5'] = df['id5'].dt.strftime('%m-%d-%Y')\n",
        "\n",
        "        # Memory optimization\n",
        "        for col in df.select_dtypes(include=['float64']):\n",
        "            df[col] = df[col].astype(np.float32)\n",
        "        for col in df.select_dtypes(include=['int64']):\n",
        "            df[col] = df[col].astype(np.int8)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def cluster_correlated_features(df: pd.DataFrame, threshold: float = 0.95):\n",
        "        \"\"\"\n",
        "        Identify groups of highly correlated features and create meta-features by averaging each cluster.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        df : pd.DataFrame\n",
        "            Input dataframe with numerical features.\n",
        "        threshold : float\n",
        "            Correlation threshold for grouping features into clusters.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        df : pd.DataFrame\n",
        "            Modified dataframe with added cluster mean features.\n",
        "        clusters : List[List[str]]\n",
        "            List of feature clusters with high correlation.\n",
        "        \"\"\"\n",
        "        assert isinstance(df, pd.DataFrame), f\"Expected DataFrame, got {type(df)}\"\n",
        "\n",
        "        # Compute absolute correlation matrix (NaN-safe)\n",
        "        corr_matrix = df.corr(numeric_only=True).abs()\n",
        "        clusters = []\n",
        "        remaining_features = set(corr_matrix.columns)\n",
        "\n",
        "        while remaining_features:\n",
        "            base_feature = remaining_features.pop()\n",
        "            correlated = corr_matrix[base_feature][corr_matrix[base_feature] > threshold].index.tolist()\n",
        "\n",
        "            if len(correlated) > 1:\n",
        "                clusters.append(correlated)\n",
        "                remaining_features -= set(correlated)\n",
        "\n",
        "        # Create meta-features\n",
        "        for cluster in clusters:\n",
        "            cluster_name = f'cluster_{cluster[0]}_mean'\n",
        "            df[cluster_name] = df[cluster].mean(axis=1)\n",
        "\n",
        "        return df, clusters\n",
        "\n",
        "\n",
        "    def display_shap_interactions(model, X_sample):\n",
        "        \"\"\"Calculate SHAP interaction values\"\"\"\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_interaction = explainer.shap_interaction_values(X_sample)\n",
        "\n",
        "        # Get top interactions\n",
        "        mean_abs_interactions = np.abs(shap_interaction).mean(0)\n",
        "        interaction_df = pd.DataFrame(mean_abs_interactions,\n",
        "                                    index=X_sample.columns,\n",
        "                                    columns=X_sample.columns)\n",
        "\n",
        "        print(\"Top feature interactions:\")\n",
        "        print(interaction_df.stack().sort_values(ascending=False).head(10))\n",
        "\n",
        "    def display_conditional_shap_importance(self, df):\n",
        "        \"\"\"Analyze feature importance conditioned on dominant feature values\"\"\"\n",
        "        if 'time_since_last_interaction' not in df.columns:\n",
        "            print(\"‚ö†Ô∏è 'time_since_last_interaction' not found for conditional SHAP\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Conditional SHAP Importance ===\")\n",
        "\n",
        "        try:\n",
        "            # Create bins safely\n",
        "            values = df['time_since_last_interaction']\n",
        "            n_bins = min(5, len(values.unique()))  # Don't create more bins than unique values\n",
        "\n",
        "            if n_bins < 2:\n",
        "                print(\"‚ö†Ô∏è Not enough unique values for conditional analysis\")\n",
        "                return\n",
        "\n",
        "            df['time_since_last_bin'] = pd.qcut(values, q=n_bins, duplicates='drop')\n",
        "\n",
        "            # Analyze each bin\n",
        "            for bin_val in sorted(df['time_since_last_bin'].unique()):\n",
        "                bin_data = df[df['time_since_last_bin'] == bin_val]\n",
        "                if len(bin_data) > 100:  # Minimum sample size\n",
        "                    print(f\"\\nüìä SHAP Importance for time bin: {bin_val} (n={len(bin_data)})\")\n",
        "                    self.display_feature_importance(bin_data, is_main_analysis=False)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Bin too small ({len(bin_data)} samples) - skipping\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Conditional SHAP failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    def reduce_memory(df):\n",
        "        for col in df.select_dtypes(include=[\"float64\"]).columns:\n",
        "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
        "        for col in df.select_dtypes(include=[\"int64\"]).columns:\n",
        "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
        "        return df\n",
        "\n",
        "    def create_transaction_features(self, df):\n",
        "        \"\"\"Create memory-safe transaction-based features with user-centric aggregations\"\"\"\n",
        "        print(\"üîÑ Creating enhanced transaction features...\")\n",
        "\n",
        "        if self.add_trans.empty:\n",
        "            df = df.assign(\n",
        "                user_avg_spend=0.0,\n",
        "                user_total_spend=0.0,\n",
        "                user_transaction_count=0,\n",
        "                user_distinct_products=0,\n",
        "                user_last_transaction_date=pd.NaT,\n",
        "                user_spend_std=0.0,\n",
        "                user_spend_skew=0.0\n",
        "            )\n",
        "            return df\n",
        "\n",
        "        trans_cols = ['id2', 'f367', 'f368', 'f370']\n",
        "        if 'id8' in self.add_trans.columns:\n",
        "            trans_cols.append('id8')\n",
        "\n",
        "        trans_data = self.add_trans[trans_cols].copy()\n",
        "        trans_data['id2'] = trans_data['id2'].astype('category')\n",
        "        df['id2'] = df['id2'].astype('category')\n",
        "\n",
        "        agg_funcs = {\n",
        "            'f367': ['count', 'sum', 'mean', 'std', 'skew'],\n",
        "            'f368': ['nunique'],\n",
        "            'f370': ['max']\n",
        "        }\n",
        "\n",
        "        print(\"üîÑ Aggregating transaction data...\")\n",
        "        user_chunks = np.array_split(trans_data['id2'].cat.categories.tolist(), 10)\n",
        "        chunk_files = []\n",
        "\n",
        "        for i, users in enumerate(user_chunks):\n",
        "            print(f\"   üß© Chunk {i+1}/10\")\n",
        "            chunk = trans_data[trans_data['id2'].isin(users)].copy()\n",
        "            chunk_agg = chunk.groupby('id2').agg(agg_funcs)\n",
        "            chunk_agg.columns = [f'{a}_{b}' for a, b in chunk_agg.columns]\n",
        "            chunk_agg = chunk_agg.reset_index()\n",
        "            chunk_file = f\"/kaggle/working/trans_chunk_{i}.feather\"\n",
        "            chunk_agg.to_feather(chunk_file)\n",
        "            chunk_files.append(chunk_file)\n",
        "\n",
        "            del chunk, chunk_agg\n",
        "            gc.collect()\n",
        "\n",
        "        # Load & combine from disk\n",
        "        all_chunks = [pd.read_feather(f) for f in chunk_files]\n",
        "        trans_agg = pd.concat(all_chunks).groupby('id2').sum().reset_index()\n",
        "        print(f\"üîç trans_agg size: {trans_agg.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "\n",
        "        trans_agg.rename(columns={\n",
        "            'f367_count': 'user_transaction_count',\n",
        "            'f367_sum': 'user_total_spend',\n",
        "            'f367_mean': 'user_avg_spend',\n",
        "            'f367_std': 'user_spend_std',\n",
        "            'f367_skew': 'user_spend_skew',\n",
        "            'f368_nunique': 'user_distinct_products',\n",
        "            'f370_max': 'user_last_transaction_date'\n",
        "        }, inplace=True)\n",
        "\n",
        "        trans_agg.to_feather(\"/kaggle/working/trans_agg.feather\")\n",
        "        del trans_data, all_chunks\n",
        "        gc.collect()\n",
        "\n",
        "        trans_agg = pd.read_feather(\"/kaggle/working/trans_agg.feather\")\n",
        "\n",
        "\n",
        "\n",
        "        trans_agg = trans_agg.astype({\n",
        "            'user_transaction_count': 'uint32',\n",
        "            'user_distinct_products': 'uint16',\n",
        "            'user_total_spend': 'float32',\n",
        "            'user_avg_spend': 'float32',\n",
        "            'user_spend_std': 'float32',\n",
        "            'user_spend_skew': 'float32'\n",
        "        })\n",
        "\n",
        "        df = df.merge(trans_agg, on='id2', how='left')\n",
        "        df = df.fillna({\n",
        "            'user_transaction_count': 0,\n",
        "            'user_distinct_products': 0,\n",
        "            'user_total_spend': 0.0,\n",
        "            'user_avg_spend': 0.0,\n",
        "            'user_spend_std': 0.0,\n",
        "            'user_spend_skew': 0.0,\n",
        "            'user_last_transaction_date': pd.NaT\n",
        "        })\n",
        "\n",
        "        del trans_agg\n",
        "        gc.collect()\n",
        "        return df\n",
        "\n",
        "    def create_historical_ctr_features(self, df):\n",
        "        \"\"\"Create historical CTR features grouped by hour, weekday, and month\"\"\"\n",
        "        print(\"üîÑ Creating historical CTR features...\")\n",
        "\n",
        "        if self.add_event.empty:\n",
        "            df['hourly_ctr'] = 0.0\n",
        "            df['weekday_ctr'] = 0.0\n",
        "            df['monthly_ctr'] = 0.0\n",
        "            return df\n",
        "\n",
        "        # Process event data\n",
        "        event_df = self.add_event.copy()\n",
        "        event_df['impression_time'] = pd.to_datetime(event_df['id4'], errors='coerce')\n",
        "        event_df['click_time'] = pd.to_datetime(event_df['id7'], errors='coerce')\n",
        "        event_df['clicked'] = event_df['click_time'].notnull().astype(np.int8)\n",
        "\n",
        "        # Extract time components\n",
        "        event_df['hour'] = event_df['impression_time'].dt.hour\n",
        "        event_df['weekday'] = event_df['impression_time'].dt.dayofweek\n",
        "        event_df['month'] = event_df['impression_time'].dt.month\n",
        "\n",
        "        # Historical CTR by hour\n",
        "        hourly_ctr = event_df.groupby('hour')['clicked'].mean().reset_index()\n",
        "        hourly_ctr.columns = ['hour', 'hourly_ctr']\n",
        "        hourly_ctr['hourly_ctr'] = hourly_ctr['hourly_ctr'].astype(np.float32)\n",
        "\n",
        "        # Historical CTR by weekday\n",
        "        weekday_ctr = event_df.groupby('weekday')['clicked'].mean().reset_index()\n",
        "        weekday_ctr.columns = ['weekday', 'weekday_ctr']\n",
        "        weekday_ctr['weekday_ctr'] = weekday_ctr['weekday_ctr'].astype(np.float32)\n",
        "\n",
        "        # Historical CTR by month\n",
        "        monthly_ctr = event_df.groupby('month')['clicked'].mean().reset_index()\n",
        "        monthly_ctr.columns = ['month', 'monthly_ctr']\n",
        "        monthly_ctr['monthly_ctr'] = monthly_ctr['monthly_ctr'].astype(np.float32)\n",
        "\n",
        "        # Merge with main dataframe\n",
        "        if 'id4_hour' in df.columns:\n",
        "            df = df.merge(hourly_ctr, left_on='id4_hour', right_on='hour', how='left')\n",
        "            df['hourly_ctr'] = df['hourly_ctr'].fillna(0).astype(np.float32)\n",
        "            df.drop('hour', axis=1, inplace=True)\n",
        "        else:\n",
        "            df['hourly_ctr'] = 0.0\n",
        "\n",
        "        if 'id4_dow' in df.columns:\n",
        "            df = df.merge(weekday_ctr, left_on='id4_dow', right_on='weekday', how='left')\n",
        "            df['weekday_ctr'] = df['weekday_ctr'].fillna(0).astype(np.float32)\n",
        "            df.drop('weekday', axis=1, inplace=True)\n",
        "        else:\n",
        "            df['weekday_ctr'] = 0.0\n",
        "\n",
        "        if 'id4_month' in df.columns:\n",
        "            df = df.merge(monthly_ctr, left_on='id4_month', right_on='month', how='left')\n",
        "            df['monthly_ctr'] = df['monthly_ctr'].fillna(0).astype(np.float32)\n",
        "            df.drop('month', axis=1, inplace=True)\n",
        "        else:\n",
        "            df['monthly_ctr'] = 0.0\n",
        "\n",
        "        # Cleanup\n",
        "        del event_df, hourly_ctr, weekday_ctr, monthly_ctr\n",
        "        gc.collect()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_industry_specific_features(self, df):\n",
        "        \"\"\"Create industry-specific metrics\"\"\"\n",
        "        print(\"üîÑ Creating industry-specific features...\")\n",
        "\n",
        "        # Industry spending proportions\n",
        "        spend_cols = [f'f{i}' for i in range(152, 163)]  # f152-f162: Debit amounts by category\n",
        "        if all(col in df.columns for col in spend_cols):\n",
        "            total_spend = df[spend_cols].sum(axis=1)\n",
        "            for i, col in enumerate(spend_cols):\n",
        "                df[f'spend_proportion_cat{i+1}'] = (df[col] / (total_spend + 1e-5)).astype(np.float32)\n",
        "\n",
        "        # Merchant CTR trends\n",
        "        if 'f137' in df.columns and 'f138' in df.columns:\n",
        "            df['merchant_ctr_trend'] = (df['f138'] - df['f137']).astype(np.float32)\n",
        "\n",
        "        # Offer category engagement\n",
        "        category_cols = [f'f{i}' for i in range(226, 310)]  # Offer category one-hot encoded features\n",
        "        if any(col in df.columns for col in category_cols):\n",
        "            present_cols = [col for col in category_cols if col in df.columns]\n",
        "            if present_cols:\n",
        "               df['primary_offer_category'] = df[present_cols].idxmax(axis=1).str.extract(r'f(\\d+)')[0].fillna(-1).astype(np.int16)\n",
        "\n",
        "        # Add to create_industry_specific_features()\n",
        "        spend_cols = [f'f{i}' for i in range(152, 163)]  # Debit amounts\n",
        "        if all(col in df.columns for col in spend_cols):\n",
        "            df['total_spend_variability'] = df[spend_cols].std(axis=1).astype(np.float32)\n",
        "            df['max_spend_category'] = (\n",
        "                pd.to_numeric(df[spend_cols].idxmax(axis=1).str.extract(r'f(\\d+)')[0], errors='coerce')\n",
        "                .fillna(-1)\n",
        "                .astype(np.int8)\n",
        "            )\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_customer_engagement_features(self, df):\n",
        "        \"\"\"Create customer engagement metrics\"\"\"\n",
        "        print(\"üîÑ Creating customer engagement features...\")\n",
        "\n",
        "        # Engagement intensity\n",
        "        if 'f59' in df.columns and 'f68' in df.columns:\n",
        "            df['engagement_intensity_30d'] = (df['f59'] / (df['f68'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "        # Page category diversity\n",
        "        if 'f13' in df.columns:\n",
        "            df['page_diversity'] = (df['f13'] / df.groupby('id2')['f13'].transform('max')).fillna(0)\n",
        "\n",
        "        # Channel engagement diversity\n",
        "        if 'f22' in df.columns:\n",
        "            df['channel_diversity'] = (df['f22'] / df.groupby('id2')['f22'].transform('max')).fillna(0)\n",
        "\n",
        "        # Email engagement ratio\n",
        "        if 'f32' in df.columns and 'f33' in df.columns:\n",
        "            df['email_engagement_ratio'] = (df['f33'] / (df['f32'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"Create enhanced interaction features with new ratios and combinations\"\"\"\n",
        "        print(\"üîÑ Creating enhanced interaction features...\")\n",
        "\n",
        "        # Ratio Features\n",
        "        if 'f33' in df.columns and 'f32' in df.columns:\n",
        "            df['user_email_ctr'] = (df['f33'] / (df['f32'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "        if 'f29' in df.columns and 'f28' in df.columns:\n",
        "            df['user_offer_specific_ctr_decayed'] = (df['f29'] / (df['f28'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "        if 'f147' in df.columns and 'f149' in df.columns:\n",
        "            df['user_recent_activity_ratio'] = (df['f149'] / (df['f147'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "\n",
        "        # Category Interest Match Features\n",
        "        category_cols = [f'f{i}' for i in range(226, 310)]  # Offer category one-hot encoded features\n",
        "        if any(col in df.columns for col in category_cols):\n",
        "            present_cols = [col for col in category_cols if col in df.columns]\n",
        "\n",
        "            # Travel category\n",
        "            if 'f232' in present_cols:\n",
        "                travel_cols = ['f1', 'f9']\n",
        "                present_travel = [col for col in travel_cols if col in df.columns]\n",
        "                if present_travel:\n",
        "                    df['interest_match_travel'] = (df['f232'] * df[present_travel].sum(axis=1)).astype(np.float32)\n",
        "\n",
        "            # Dining category\n",
        "            if 'f227' in present_cols:\n",
        "                dining_cols = ['f8', 'f2']\n",
        "                present_dining = [col for col in dining_cols if col in df.columns]\n",
        "                if present_dining:\n",
        "                    df['interest_match_dining'] = (df['f227'] * df[present_dining].sum(axis=1)).astype(np.float32)\n",
        "\n",
        "        # Membership x Spend interaction\n",
        "        if 'f42' in df.columns and 'f39' in df.columns:\n",
        "            df['membership_spend_interaction'] = (df['f42'] * df['f39']).astype(np.float32)\n",
        "\n",
        "        # Email engagement difference\n",
        "        if 'f33' in df.columns and 'f34' in df.columns:\n",
        "            df['net_email_engagement'] = (df['f33'] - df['f34']).astype(np.float32)\n",
        "\n",
        "        # Ensure datetime format\n",
        "        df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n",
        "        df['user_last_transaction_date'] = pd.to_datetime(df['user_last_transaction_date'], errors='coerce')\n",
        "\n",
        "        # Time since last transaction\n",
        "        if 'user_last_transaction_date' in df.columns and 'id4' in df.columns:\n",
        "            df['time_since_user_last_tx'] = (\n",
        "                (df['id4'] - df['user_last_transaction_date']).dt.total_seconds() / (24 * 3600)\n",
        "            ).astype(np.float32)\n",
        "\n",
        "        # Add to create_interaction_features()\n",
        "        if 'time_since_last_interaction' in df.columns and 'user_global_ctr' in df.columns:\n",
        "            df['time_ctr_interaction'] = (df['time_since_last_interaction'] * df['user_global_ctr']).astype(np.float32)\n",
        "\n",
        "        if 'f316' in df.columns and 'f345' in df.columns:\n",
        "            df['f316_f345_ratio'] = (df['f316'] / (df['f345'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_ctr_features(self, df, is_training=True):\n",
        "        \"\"\"Create CTR features from event data with offer-centric aggregations\"\"\"\n",
        "        print(\"‚ö° Creating CTR features...\")\n",
        "\n",
        "        if self.add_event.empty:\n",
        "            # Initialize all CTR-related features\n",
        "            ctr_features = [\n",
        "                'user_global_ctr', 'user_total_impressions', 'user_total_clicks',\n",
        "                'user_distinct_offers_seen', 'offer_global_ctr',\n",
        "                'offer_total_impressions', 'offer_total_clicks',\n",
        "                'offer_unique_users_impressed'\n",
        "            ]\n",
        "            for feat in ctr_features:\n",
        "                df[feat] = 0.0 if 'ctr' in feat else 0\n",
        "            return df\n",
        "\n",
        "        # Process event data\n",
        "        event_df = self.add_event.copy()\n",
        "        event_df['impression_time'] = pd.to_datetime(event_df['id4'], errors='coerce')\n",
        "        event_df['click_time'] = pd.to_datetime(event_df['id7'], errors='coerce')\n",
        "        event_df['clicked'] = event_df['click_time'].notnull().astype(np.int8)\n",
        "\n",
        "        # User-centric CTR features\n",
        "        user_agg = event_df.groupby('id2').agg({\n",
        "            'clicked': ['sum', 'count'],\n",
        "            'id3': ['nunique']\n",
        "        }).reset_index()\n",
        "        user_agg.columns = ['id2', 'user_total_clicks', 'user_total_impressions', 'user_distinct_offers_seen']\n",
        "        user_agg['user_global_ctr'] = (user_agg['user_total_clicks'] / user_agg['user_total_impressions']).astype(np.float32)\n",
        "\n",
        "        # Offer-centric CTR features\n",
        "        offer_agg = event_df.groupby('id3').agg({\n",
        "            'clicked': ['sum', 'count'],\n",
        "            'id2': ['nunique']\n",
        "        }).reset_index()\n",
        "        offer_agg.columns = ['id3', 'offer_total_clicks', 'offer_total_impressions', 'offer_unique_users_impressed']\n",
        "        offer_agg['offer_global_ctr'] = (offer_agg['offer_total_clicks'] / offer_agg['offer_total_impressions']).astype(np.float32)\n",
        "\n",
        "        # Ensure consistent dtype for merge keys\n",
        "        user_agg['id2'] = user_agg['id2'].astype(str)\n",
        "        offer_agg['id3'] = offer_agg['id3'].astype(str)\n",
        "        df['id2'] = df['id2'].astype(str)\n",
        "        df['id3'] = df['id3'].astype(str)\n",
        "\n",
        "        # Merge CTR features\n",
        "        df = df.merge(user_agg, on='id2', how='left')\n",
        "        df = df.merge(offer_agg, on='id3', how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        ctr_cols = ['user_global_ctr', 'offer_global_ctr']\n",
        "        count_cols = ['user_total_impressions', 'user_total_clicks', 'user_distinct_offers_seen',\n",
        "                     'offer_total_impressions', 'offer_total_clicks', 'offer_unique_users_impressed']\n",
        "\n",
        "        df[ctr_cols] = df[ctr_cols].fillna(0)\n",
        "        df[count_cols] = df[count_cols].fillna(0)\n",
        "\n",
        "        df['ctr'] = 0.6 * df['user_global_ctr'] + 0.4 * df['offer_global_ctr']\n",
        "\n",
        "\n",
        "        # Clean up\n",
        "        del event_df, user_agg, offer_agg\n",
        "        gc.collect()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_offer_metadata_features(self, df):\n",
        "        \"\"\"Memory-optimized offer metadata features\"\"\"\n",
        "        print(\"üîÑ Creating offer metadata features...\")\n",
        "\n",
        "        if self.offer_metadata.empty:\n",
        "            return df\n",
        "\n",
        "        # Select only needed columns\n",
        "        meta_cols = ['id3', 'f376']\n",
        "        if 'id8' in self.offer_metadata.columns:\n",
        "            meta_cols.append('id8')\n",
        "        if 'id10' in self.offer_metadata.columns:\n",
        "            meta_cols.append('id10')\n",
        "        if 'id11' in self.offer_metadata.columns:\n",
        "            meta_cols.append('id11')\n",
        "\n",
        "        meta_df = self.offer_metadata[meta_cols].copy()\n",
        "\n",
        "        # Merge with main df\n",
        "\n",
        "        # Coerce both columns to same type (int or str depending on actual data)\n",
        "        df['id3'] = df['id3'].astype(str)\n",
        "        meta_df['id3'] = meta_df['id3'].astype(str)\n",
        "\n",
        "        # Now safe to merge\n",
        "        df = df.merge(meta_df, on='id3', how='left')\n",
        "\n",
        "\n",
        "        # Create discount features if available\n",
        "        if 'f376' in df.columns:\n",
        "            df['discount_bin'] = pd.cut(\n",
        "                df['f376'],\n",
        "                bins=[-np.inf, 0.1, 0.2, 0.3, np.inf],\n",
        "                labels=[1, 2, 3, 4]\n",
        "            ).cat.codes.astype('int8')\n",
        "            df['log_discount'] = np.log1p(df['f376']).astype('float32')\n",
        "\n",
        "        # Create industry features if possible\n",
        "        if 'id8' in df.columns and 'user_global_ctr' in df.columns and 'offer_global_ctr' in df.columns:\n",
        "            # Create composite CTR with minimal memory\n",
        "            df['ctr'] = (0.6 * df['user_global_ctr'] + 0.4 * df['offer_global_ctr']).astype('float32')\n",
        "\n",
        "            # Calculate industry affinity in chunks if large\n",
        "            if len(df) > 1e6:\n",
        "                chunks = []\n",
        "                for chunk in np.array_split(df[['id2', 'id8', 'ctr']], 10):\n",
        "                    chunk['industry_affinity'] = chunk.groupby(['id2', 'id8'])['ctr'].transform('mean')\n",
        "                    chunks.append(chunk[['industry_affinity']])\n",
        "                industry_affinity = pd.concat(chunks)\n",
        "                df['industry_affinity'] = industry_affinity['industry_affinity'].fillna(0).astype('float32')\n",
        "            else:\n",
        "                df['industry_affinity'] = df.groupby(['id2', 'id8'])['ctr'].transform('mean').fillna(0).astype('float32')\n",
        "\n",
        "            # Industry discount interaction\n",
        "            if 'f376' in df.columns:\n",
        "                df['industry_discount_affinity'] = (df['industry_affinity'] * df['f376']).astype('float32')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_user_offer_interaction_features(self, df):\n",
        "        \"\"\"Memory-optimized interaction features\"\"\"\n",
        "        print(\"üöÄ Creating user-offer interaction features...\")\n",
        "\n",
        "        if self.add_event.empty or self.add_trans.empty:\n",
        "            return df.assign(\n",
        "                user_ctr_for_brand=0.0,\n",
        "                user_ctr_for_industry=0.0,\n",
        "                user_spend_in_this_offer_industry=0.0,\n",
        "                user_tx_count_in_this_offer_industry=0,\n",
        "                days_from_offer_start=0.0,\n",
        "                days_until_offer_end=0.0,\n",
        "                offer_progress_pct=0.0\n",
        "            )\n",
        "\n",
        "        # Ensure we have required metadata\n",
        "        if 'id10' not in df.columns or 'id11' not in df.columns:\n",
        "            df = self.create_offer_metadata_features(df)\n",
        "\n",
        "        # Process in chunks if large\n",
        "        chunk_size = 500000\n",
        "        if len(df) > chunk_size:\n",
        "            chunks = []\n",
        "            for i in range(0, len(df), chunk_size):\n",
        "                chunk = self._process_interaction_chunk(df.iloc[i:i+chunk_size].copy())\n",
        "                chunks.append(chunk)\n",
        "                gc.collect()\n",
        "            return pd.concat(chunks)\n",
        "        else:\n",
        "            return self._process_interaction_chunk(df)\n",
        "\n",
        "    def _process_interaction_chunk(self, df):\n",
        "        \"\"\"Process a chunk of data for interaction features\"\"\"\n",
        "        # 1. Brand CTR features\n",
        "        if 'id11' in df.columns:\n",
        "            # Calculate brand CTR (using pre-aggregated stats if possible)\n",
        "            brand_ctr = self._get_brand_ctr_stats()\n",
        "            df = df.merge(brand_ctr, on=['id2', 'id11'], how='left')\n",
        "            df['user_ctr_for_brand'] = df['user_ctr_for_brand'].fillna(0).astype('float32')\n",
        "\n",
        "            # 2. Industry CTR features\n",
        "        if 'id10' in df.columns:\n",
        "            industry_ctr = self._get_industry_ctr_stats()\n",
        "            # Ensure 'id2' and 'id10' are the same type in both dataframes before merge\n",
        "            df['id2'] = df['id2'].astype(str)\n",
        "            industry_ctr['id2'] = industry_ctr['id2'].astype(str)\n",
        "\n",
        "            df['id10'] = df['id10'].astype(str)\n",
        "            industry_ctr['id10'] = industry_ctr['id10'].astype(str)\n",
        "\n",
        "            # Now safe to merge\n",
        "            df = df.merge(industry_ctr, on=['id2', 'id10'], how='left')\n",
        "\n",
        "            df['user_ctr_for_industry'] = df['user_ctr_for_industry'].fillna(0).astype('float32')\n",
        "\n",
        "        # 3. Transaction-industry features\n",
        "        if 'id10' in df.columns and hasattr(self, '_industry_spend_stats'):\n",
        "            industry_spend = self._industry_spend_stats\n",
        "            df = df.merge(industry_spend, left_on=['id2', 'id10'], right_on=['id2', 'id8'], how='left')\n",
        "            df['user_spend_in_this_offer_industry'] = df['user_spend_in_industry'].fillna(0).astype('float32')\n",
        "            df['user_tx_count_in_this_offer_industry'] = df['user_tx_count_in_industry'].fillna(0).astype('int32')\n",
        "            df.drop(['user_spend_in_industry', 'user_tx_count_in_industry', 'id8'],\n",
        "                  axis=1, errors='ignore', inplace=True)\n",
        "\n",
        "        # 4. Time-based features\n",
        "        time_cols = ['id4', 'id12', 'id13']\n",
        "        if all(col in df.columns for col in time_cols):\n",
        "            df['days_from_offer_start'] = (df['id4'] - df['id12']).dt.total_seconds() / 86400\n",
        "            df['days_until_offer_end'] = (df['id13'] - df['id4']).dt.total_seconds() / 86400\n",
        "            df['offer_progress_pct'] = (df['days_from_offer_start'] /\n",
        "                                      (df['days_from_offer_start'] + df['days_until_offer_end'] + 1e-5)\n",
        "                                      ).astype('float32')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _get_brand_ctr_stats(self):\n",
        "        \"\"\"Pre-compute brand CTR stats\"\"\"\n",
        "        if not hasattr(self, '_brand_ctr_stats'):\n",
        "            if self.add_event.empty:\n",
        "                self._brand_ctr_stats = pd.DataFrame(columns=['id2', 'id11', 'user_ctr_for_brand'])\n",
        "            else:\n",
        "                event_cols = ['id2', 'id3', 'id7']\n",
        "                event_df = self.add_event[event_cols].copy()\n",
        "                event_df['clicked'] = ~pd.isna(event_df['id7'])\n",
        "\n",
        "                # Merge with brand info\n",
        "                if 'id11' in self.offer_metadata.columns:\n",
        "                    event_df = event_df.merge(\n",
        "                        self.offer_metadata[['id3', 'id11']].drop_duplicates(),\n",
        "                        on='id3',\n",
        "                        how='left'\n",
        "                    )\n",
        "                    # Calculate brand CTR\n",
        "                    brand_ctr = event_df.groupby(['id2', 'id11'])['clicked'].agg(['sum', 'count']).reset_index()\n",
        "                    brand_ctr['user_ctr_for_brand'] = (brand_ctr['sum'] / brand_ctr['count']).astype('float32')\n",
        "                    self._brand_ctr_stats = brand_ctr[['id2', 'id11', 'user_ctr_for_brand']]\n",
        "        return self._brand_ctr_stats\n",
        "\n",
        "    def _get_industry_ctr_stats(self):\n",
        "        \"\"\"Pre-compute industry CTR stats\"\"\"\n",
        "        if not hasattr(self, '_industry_ctr_stats'):\n",
        "            if self.add_event.empty:\n",
        "                self._industry_ctr_stats = pd.DataFrame(columns=['id2', 'id10', 'user_ctr_for_industry'])\n",
        "            else:\n",
        "                event_cols = ['id2', 'id3', 'id7']\n",
        "                event_df = self.add_event[event_cols].copy()\n",
        "                event_df['clicked'] = ~pd.isna(event_df['id7'])\n",
        "\n",
        "                # Merge with industry info\n",
        "                if 'id10' in self.offer_metadata.columns:\n",
        "                    event_df = event_df.merge(\n",
        "                        self.offer_metadata[['id3', 'id10']].drop_duplicates(),\n",
        "                        on='id3',\n",
        "                        how='left'\n",
        "                    )\n",
        "                    # Calculate industry CTR\n",
        "                    industry_ctr = event_df.groupby(['id2', 'id10'])['clicked'].agg(['sum', 'count']).reset_index()\n",
        "                    industry_ctr['user_ctr_for_industry'] = (industry_ctr['sum'] / industry_ctr['count']).astype('float32')\n",
        "                    self._industry_ctr_stats = industry_ctr[['id2', 'id10', 'user_ctr_for_industry']]\n",
        "        return self._industry_ctr_stats\n",
        "\n",
        "    def create_ranking_features(self, df):\n",
        "        \"\"\"Enhanced ranking features incorporating:\n",
        "        - Listwise ranking optimization (SIGIR 2023)\n",
        "        - Temporal position bias correction (KDD 2022)\n",
        "        - Session-aware context modeling (RecSys 2022)\n",
        "        - Multi-armed bandit exploration (WSDM 2023)\n",
        "        - Neural scoring integration (CIKM 2022)\n",
        "        \"\"\"\n",
        "        print(\"üöÄ Creating SOTA ranking features...\")\n",
        "\n",
        "        # 1. Session context with temporal decay (RecSys 2022)\n",
        "        if 'id4' in df.columns and 'id2' in df.columns:\n",
        "            # Create session IDs with 30-minute inactivity threshold\n",
        "            df = df.sort_values(['id2', 'id4'])\n",
        "            df['time_since_last'] = df.groupby('id2')['id4'].diff().dt.total_seconds()/60\n",
        "            df['new_session'] = (df['time_since_last'] > 30) | (df['id2'] != df['id2'].shift())\n",
        "            df['session_id'] = df.groupby('id2')['new_session'].cumsum()\n",
        "\n",
        "            # Session-level statistics\n",
        "            session_stats = df.groupby(['id2', 'session_id']).agg({\n",
        "                'id4': ['min', 'max'],\n",
        "                'id3': 'count'\n",
        "            }).reset_index()\n",
        "            session_stats.columns = ['id2', 'session_id', 'session_start', 'session_end', 'session_length']\n",
        "            df = df.merge(session_stats, on=['id2', 'session_id'], how='left')\n",
        "\n",
        "            # Position bias correction (KDD 2022)\n",
        "\n",
        "\n",
        "        # 2. Listwise ranking features (SIGIR 2023)\n",
        "        if 'f376' in df.columns:\n",
        "            # Normalized discount rate\n",
        "            df['discount_norm'] = (df['f376'] - df['f376'].min()) / \\\n",
        "                                 (df['f376'].max() - df['f376'].min() + 1e-5)\n",
        "\n",
        "            # Combined utility score\n",
        "            utility_components = []\n",
        "            if 'offer_global_ctr' in df.columns:\n",
        "                utility_components.append(0.4 * df['offer_global_ctr'])\n",
        "            if 'user_ctr_for_brand' in df.columns:\n",
        "                utility_components.append(0.3 * df['user_ctr_for_brand'])\n",
        "            if 'discount_norm' in df.columns:\n",
        "                utility_components.append(0.2 * df['discount_norm'])\n",
        "            if 'position_bias' in df.columns:\n",
        "                utility_components.append(0.1 * df['position_bias'])\n",
        "\n",
        "            if utility_components:\n",
        "                df['listwise_utility'] = sum(utility_components).astype(np.float32)\n",
        "                # Listwise Rank (can have float ranks)\n",
        "                df['listwise_rank'] = (\n",
        "                    df.groupby(['id2', 'session_id'])['listwise_utility']\n",
        "                    .rank(method='dense', ascending=False)\n",
        "                    .fillna(-1.0)  # you can change this depending on meaning of NaN\n",
        "                    .astype(np.float32)\n",
        "                )\n",
        "\n",
        "        # 3. Temporal dynamics (ICML 2023)\n",
        "        if 'id4' in df.columns:\n",
        "            # Time-sensitive popularity\n",
        "            # Ensure pd.Timestamp.now() is timezone-naive (recommended for consistency)\n",
        "            # Ensure 'id4' column is parsed as timezone-naive datetime\n",
        "            df['id4'] = pd.to_datetime(df['id4'], utc=True).dt.tz_convert(None)\n",
        "\n",
        "            # Ensure 'now' is timezone-naive\n",
        "            now = pd.Timestamp.utcnow().replace(tzinfo=None)\n",
        "\n",
        "            # Perform the subtraction safely\n",
        "            df['hours_since_impression'] = (now - df['id4']).dt.total_seconds() / 3600\n",
        "\n",
        "            df['time_decay'] = np.exp(-df['hours_since_impression']/168)  # 1-week half-life\n",
        "\n",
        "            if 'offer_total_clicks' in df.columns:\n",
        "                df['time_weighted_popularity'] = (df['offer_total_clicks'] * df['time_decay']).astype(np.float32)\n",
        "                # Popularity Rank (also float)\n",
        "                df['popularity_rank'] = (\n",
        "                    df.groupby('id3')['time_weighted_popularity']\n",
        "                    .rank(method='dense', ascending=False)\n",
        "                    .fillna(-1.0)\n",
        "                    .astype(np.float32)\n",
        "                )\n",
        "\n",
        "        # 4. Exploration-exploitation features (WSDM 2023)\n",
        "        if 'offer_total_impressions' in df.columns:\n",
        "            # Upper Confidence Bound (UCB) component\n",
        "            total_impressions = df['offer_total_impressions'].sum()\n",
        "            df['exploration_bonus'] = np.sqrt(2 * np.log(total_impressions) /\n",
        "                                     (df['offer_total_impressions'] + 1e-5)).astype(np.float32)\n",
        "\n",
        "            if 'offer_global_ctr' in df.columns:\n",
        "                df['ucb_score'] = (df['offer_global_ctr'] + 0.1 * df['exploration_bonus']).astype(np.float32)\n",
        "\n",
        "        # 5. Neural scoring integration (CIKM 2022)\n",
        "        if 'user_embed_0' in df.columns and 'item_embed_0' in df.columns:\n",
        "            # Simple dot product similarity\n",
        "            embed_cols = [c for c in df.columns if 'user_embed_' in c or 'item_embed_' in c]\n",
        "            for i in range(len(embed_cols)//2):\n",
        "                df[f'embed_sim_{i}'] = (df[f'user_embed_{i}'] * df[f'item_embed_{i}']).astype(np.float32)\n",
        "\n",
        "            if 'embed_sim_0' in df.columns:\n",
        "                df['neural_match_score'] = df[[c for c in df.columns if 'embed_sim_' in c]] \\\n",
        "                                         .mean(axis=1).astype(np.float32)\n",
        "\n",
        "        # 6. Diversity features (RecSys 2022)\n",
        "        if 'id10' in df.columns and 'id2' in df.columns:\n",
        "            # Industry diversity within session\n",
        "            session_diversity = df.groupby(['id2', 'session_id'])['id10'].nunique().reset_index()\n",
        "            session_diversity.columns = ['id2', 'session_id', 'session_diversity']\n",
        "            df = df.merge(session_diversity, on=['id2', 'session_id'], how='left')\n",
        "\n",
        "            # Distance from user's preferred industries\n",
        "            if 'user_industry_affinity' in df.columns:\n",
        "                df['industry_distance'] = (1 - df['user_industry_affinity']).astype(np.float32)\n",
        "\n",
        "        # 7. Final ranking score composition\n",
        "        ranking_components = []\n",
        "        if 'listwise_utility' in df.columns:\n",
        "            ranking_components.append(0.5 * df['listwise_utility'])\n",
        "        if 'ucb_score' in df.columns:\n",
        "            ranking_components.append(0.3 * df['ucb_score'])\n",
        "        if 'neural_match_score' in df.columns:\n",
        "            ranking_components.append(0.2 * df['neural_match_score'])\n",
        "\n",
        "        if ranking_components:\n",
        "            df['final_ranking_score'] = sum(ranking_components).astype(np.float32)\n",
        "            # Final Rank (you want integer here, so be careful)\n",
        "            df['final_rank'] = (\n",
        "                df.groupby(['id2', 'session_id'])['final_ranking_score']\n",
        "                .rank(method='first', ascending=False)\n",
        "                .fillna(-1)  # Important: cannot cast NaN directly to int\n",
        "                .astype(np.int16)\n",
        "            )\n",
        "\n",
        "        # Clean up temporary columns\n",
        "        for col in ['time_since_last', 'new_session']:\n",
        "            if col in df.columns:\n",
        "                del df[col]\n",
        "\n",
        "        gc.collect()\n",
        "        return df\n",
        "\n",
        "    def generate_variable_importance_report(self, df, model, filename=\"variable_importance_report.xlsx\"):\n",
        "        \"\"\"\n",
        "        Generates a comprehensive Excel report with variable importance and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            from openpyxl import Workbook\n",
        "            from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "            from openpyxl.styles import Font, PatternFill\n",
        "\n",
        "            print(f\"\\nüìä Generating variable importance report: {filename}\")\n",
        "\n",
        "            # Get SHAP values\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            X_sample = df[self.feature_cols].sample(n=min(10000, len(df)), random_state=42).fillna(0)\n",
        "            shap_values = explainer.shap_values(X_sample)[1]  # For class 1\n",
        "            mean_abs_shap = pd.Series(np.abs(shap_values).mean(axis=0), index=X_sample.columns)\n",
        "\n",
        "            # Create comprehensive DataFrame\n",
        "            report_df = pd.DataFrame({\n",
        "                'feature': mean_abs_shap.index,\n",
        "                'shap_importance': mean_abs_shap.values,\n",
        "                'shap_importance_pct': (mean_abs_shap / mean_abs_shap.sum()) * 100,\n",
        "                'data_type': df[mean_abs_shap.index].dtypes.astype(str),\n",
        "                'missing_pct': (df[mean_abs_shap.index].isna().mean() * 100),\n",
        "                'unique_values': df[mean_abs_shap.index].nunique(),\n",
        "                'mean_value': df[mean_abs_shap.index].mean(),\n",
        "                'std_dev': df[mean_abs_shap.index].std(),\n",
        "                'min_value': df[mean_abs_shap.index].min(),\n",
        "                'max_value': df[mean_abs_shap.index].max()\n",
        "            }).sort_values('shap_importance', ascending=False)\n",
        "\n",
        "            # Add feature categories\n",
        "            def categorize_feature(feature):\n",
        "                if feature.startswith('f'): return 'numerical_feature'\n",
        "                elif feature.startswith('id'): return 'id_feature'\n",
        "                elif 'target_enc' in feature: return 'encoded_feature'\n",
        "                elif 'cluster_' in feature: return 'feature_cluster'\n",
        "                elif any(x in feature for x in ['sin', 'cos']): return 'cyclical_feature'\n",
        "                elif any(x in feature for x in ['ctr', 'rate', 'ratio']): return 'performance_metric'\n",
        "                elif any(x in feature for x in ['time', 'day', 'hour']): return 'temporal_feature'\n",
        "                else: return 'other_feature'\n",
        "\n",
        "            report_df['feature_category'] = report_df['feature'].apply(categorize_feature)\n",
        "\n",
        "            # Create Excel writer\n",
        "            writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
        "            report_df.to_excel(writer, sheet_name='Variable Importance', index=False)\n",
        "\n",
        "            # Get workbook and worksheet for styling\n",
        "            workbook = writer.book\n",
        "            worksheet = workbook['Variable Importance']\n",
        "\n",
        "            # Apply styling\n",
        "            header_font = Font(bold=True, color=\"FFFFFF\")\n",
        "            header_fill = PatternFill(start_color=\"4F81BD\", end_color=\"4F81BD\", fill_type=\"solid\")\n",
        "\n",
        "            for cell in worksheet[1]:\n",
        "                cell.font = header_font\n",
        "                cell.fill = header_fill\n",
        "\n",
        "            # Auto-adjust column widths\n",
        "            for column in worksheet.columns:\n",
        "                max_length = 0\n",
        "                column = [cell for cell in column]\n",
        "                for cell in column:\n",
        "                    try:\n",
        "                        if len(str(cell.value)) > max_length:\n",
        "                            max_length = len(str(cell.value))\n",
        "                    except:\n",
        "                        pass\n",
        "                adjusted_width = (max_length + 2)\n",
        "                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "            # Add summary statistics sheet\n",
        "            summary_df = report_df.groupby('feature_category').agg({\n",
        "                'shap_importance': 'sum',\n",
        "                'feature': 'count'\n",
        "            }).rename(columns={'shap_importance': 'total_importance', 'feature': 'count'})\n",
        "            summary_df['importance_per_feature'] = summary_df['total_importance'] / summary_df['count']\n",
        "            summary_df.to_excel(writer, sheet_name='Summary by Category')\n",
        "\n",
        "            writer.close()\n",
        "            print(f\"‚úÖ Successfully generated report: {filename}\")\n",
        "\n",
        "            return report_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to generate report: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "\n",
        "    # Add this entire method to your AMEXClickPredictionPipeline class\n",
        "\n",
        "    def display_feature_importance(self, df: pd.DataFrame, is_main_analysis=True):\n",
        "        \"\"\"Display feature importance and generate Excel report\"\"\"\n",
        "        print(\"üìä Calculating feature importance using SHAP...\")\n",
        "\n",
        "        # Only calculate SHAP if we have enough data\n",
        "        if len(df) < 100:\n",
        "            print(\"‚ö†Ô∏è Insufficient data for SHAP analysis\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            if 'lightgbm_ensemble' in self.models and self.models['lightgbm_ensemble']:\n",
        "                # Get a sample of the data for efficiency\n",
        "                sample_size = min(10000, len(df))\n",
        "                X_sample = df[self.feature_cols].sample(n=sample_size, random_state=42).fillna(0)\n",
        "\n",
        "                # Calculate SHAP values\n",
        "                explainer = shap.TreeExplainer(self.models['lightgbm_ensemble'][0])\n",
        "                shap_values = explainer.shap_values(X_sample)[1]  # For class 1\n",
        "\n",
        "                # Calculate importance\n",
        "                mean_abs_shap = pd.Series(np.abs(shap_values).mean(axis=0), index=X_sample.columns)\n",
        "                total_importance = mean_abs_shap.sum()\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'feature': mean_abs_shap.index,\n",
        "                    'shap_importance': mean_abs_shap.values,\n",
        "                    'impact_percentage': (mean_abs_shap / total_importance) * 100\n",
        "                }).sort_values('impact_percentage', ascending=False)\n",
        "\n",
        "                # Display top features\n",
        "                print(\"\\n\" + \"=\"*55)\n",
        "                print(\"üèÜ Top 20 Most Important Features (based on SHAP value)\")\n",
        "                print(\"=\"*55)\n",
        "                print(f\"{'Feature':<40} | {'Impact (%)'}\")\n",
        "                print(\"-\"*55)\n",
        "                for _, row in importance_df.head(20).iterrows():\n",
        "                    print(f\"{row['feature']:<40} | {row['impact_percentage']:.2f}%\")\n",
        "                print(\"=\"*55 + \"\\n\")\n",
        "\n",
        "                # Generate full report only for main analysis\n",
        "                if is_main_analysis:\n",
        "                    self.generate_variable_importance_report(df, self.models['lightgbm_ensemble'][0])\n",
        "\n",
        "                # # Only run conditional analysis for main dataset\n",
        "                # if is_main_analysis and 'time_since_last_interaction' in df.columns:\n",
        "                #     self.display_conditional_shap_importance(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Feature importance analysis failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    def apply_variance_threshold(self, df, is_training=True):\n",
        "        \"\"\"Apply variance threshold to remove near-constant features\"\"\"\n",
        "        print(\"üîÑ Applying variance threshold...\")\n",
        "\n",
        "        if is_training:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            if 'y' in numeric_cols:\n",
        "                numeric_cols.remove('y')\n",
        "\n",
        "            X = df[numeric_cols].fillna(0)\n",
        "            self.variance_selector = VarianceThreshold(threshold=1e-5)\n",
        "            X_var = self.variance_selector.fit_transform(X)\n",
        "\n",
        "            # Get selected features\n",
        "            selected_mask = self.variance_selector.get_support()\n",
        "            selected_features = [col for col, selected in zip(numeric_cols, selected_mask) if selected]\n",
        "\n",
        "            print(f\"‚úÖ Variance threshold removed {len(numeric_cols) - len(selected_features)} near-constant features\")\n",
        "\n",
        "            # Save variance selector\n",
        "            joblib.dump(self.variance_selector, f\"{self.temp_dir}variance_selector.pkl\")\n",
        "\n",
        "            return selected_features\n",
        "        else:\n",
        "            # Load variance selector for test data\n",
        "            if os.path.exists(f\"{self.temp_dir}variance_selector.pkl\"):\n",
        "                self.variance_selector = joblib.load(f\"{self.temp_dir}variance_selector.pkl\")\n",
        "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "                selected_mask = self.variance_selector.get_support()\n",
        "                selected_features = [col for col, selected in zip(numeric_cols, selected_mask) if selected]\n",
        "                return selected_features\n",
        "            else:\n",
        "                return df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    def _process_feature_chunk(self, df, is_training):\n",
        "        if is_training:\n",
        "            df = self._process_target(df)\n",
        "\n",
        "        print(\"Step 1: Time features\")\n",
        "        df = self.create_time_features(df)\n",
        "\n",
        "        # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 2: Target encoding\")\n",
        "        cat_features = ['id3', 'id6', 'f42', 'f48', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57']\n",
        "        df = self.create_target_encoding(df, cat_features, 'y', is_training)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 3: CTR features\")\n",
        "        df = self.create_ctr_features(df, is_training)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 4: Historical CTR features\")\n",
        "        df = self.create_historical_ctr_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 5: Offer metadata features\")\n",
        "        df = self.create_offer_metadata_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 6: Transaction features\")\n",
        "        df = self.create_transaction_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 7: User-offer interaction features\")\n",
        "        df = self.create_user_offer_interaction_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 8: Customer engagement features\")\n",
        "        df = self.create_customer_engagement_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 9: Industry-specific features\")\n",
        "        df = self.create_industry_specific_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 10: Ranking features\")\n",
        "        df = self.create_ranking_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        print(\"Step 11: Interaction features\")\n",
        "        df = self.create_interaction_features(df)\n",
        "         # Get memory info\n",
        "        virtual_mem = psutil.virtual_memory()\n",
        "\n",
        "        # Available and used memory in bytes\n",
        "        available_ram = virtual_mem.available\n",
        "        used_ram = virtual_mem.used\n",
        "        total_ram = virtual_mem.total\n",
        "\n",
        "        # Convert to human-readable format (e.g., GB)\n",
        "        print(f\"üíæ Total RAM:     {humanize.naturalsize(total_ram)}\")\n",
        "        print(f\"‚úÖ Available RAM: {humanize.naturalsize(available_ram)}\")\n",
        "        print(f\"üì¶ Used RAM:      {humanize.naturalsize(used_ram)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_features(self, df, is_training=True):\n",
        "        \"\"\"Complete + memory-efficient feature preparation pipeline\"\"\"\n",
        "        print(\"üîß Starting feature preparation...\")\n",
        "\n",
        "        # For large data: process in chunks\n",
        "        if len(df) > 1_000_000:\n",
        "            chunks = []\n",
        "            for i in range(0, len(df), 500_000):\n",
        "                print(f\"üîß Processing chunk {i//500_000 + 1}/{(len(df)//500_000)+1}\")\n",
        "                chunk = df.iloc[i:i+500_000].copy()\n",
        "                chunk = self._process_feature_chunk(chunk, is_training)\n",
        "                chunks.append(chunk)\n",
        "                del chunk\n",
        "                gc.collect()\n",
        "            df = pd.concat(chunks, axis=0)\n",
        "            del chunks\n",
        "            gc.collect()\n",
        "        else:\n",
        "            df = self._process_feature_chunk(df, is_training)\n",
        "\n",
        "        is_training=True\n",
        "\n",
        "\n",
        "        # Feature selection (only for training)\n",
        "        if is_training and 'y' in df.columns:\n",
        "            print(\"üß† Performing feature selection...\")\n",
        "\n",
        "            # Step 1: Apply variance threshold\n",
        "            selected_features = self.apply_variance_threshold(df, is_training)\n",
        "\n",
        "            # Step 2: Prepare matrix\n",
        "            X = df[selected_features].fillna(0)\n",
        "            y = df['y']\n",
        "\n",
        "            # Step 3: RandomForest-based selection\n",
        "            clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "            clf.fit(X, y)\n",
        "\n",
        "            selector = SelectFromModel(clf, threshold=\"median\", prefit=True)\n",
        "            X_selected = selector.transform(X)\n",
        "\n",
        "            # Step 4: Save selected feature names\n",
        "            self.feature_cols = X.columns[selector.get_support()].tolist()\n",
        "            print(f\"‚úÖ Selected {len(self.feature_cols)} features using RandomForest importance\")\n",
        "\n",
        "            # Save selector\n",
        "            joblib.dump(self.feature_selector, f\"{self.temp_dir}feature_selector.pkl\")\n",
        "\n",
        "        elif not is_training:\n",
        "            if os.path.exists(f\"{self.temp_dir}feature_selector.pkl\"):\n",
        "                self.feature_selector = joblib.load(f\"{self.temp_dir}feature_selector.pkl\")\n",
        "\n",
        "                if self.feature_selector is not None:\n",
        "                    selected_features = self.apply_variance_threshold(df, is_training)\n",
        "                    X = df[selected_features].fillna(0)\n",
        "                    selected_mask = self.feature_selector.get_support()\n",
        "                    self.feature_cols = [col for col, selected in zip(selected_features, selected_mask) if selected]\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è feature_selector.pkl exists but is empty or corrupted ‚Äî falling back to all numeric features\")\n",
        "                    self.feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No selector found ‚Äî using all numeric features\")\n",
        "                self.feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        gc.collect()\n",
        "        return df\n",
        "\n",
        "    def _split_train_val_by_time(self, df, date_col='id5_dt', cutoff_date=None):\n",
        "        \"\"\"\n",
        "        Split train/val based on a time cutoff on date_col.\n",
        "        Ensures both splits are non-empty.\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Creating time-based train-validation split...\")\n",
        "\n",
        "        # Ensure date format\n",
        "        if df[date_col].dtype == object:\n",
        "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "        # Use provided cutoff_date or calculate it\n",
        "        if cutoff_date is None:\n",
        "            cutoff = df[date_col].quantile(0.8)\n",
        "        else:\n",
        "            cutoff = pd.to_datetime(cutoff_date)\n",
        "\n",
        "        # Split\n",
        "        train_split = df[df[date_col] < cutoff].copy()\n",
        "        val_split = df[df[date_col] >= cutoff].copy()\n",
        "\n",
        "        # Validate non-empty\n",
        "        if len(train_split) == 0 or len(val_split) == 0:\n",
        "            raise ValueError(f\"‚ùå Invalid split: train={len(train_split)} val={len(val_split)}. Try adjusting cutoff date.\")\n",
        "\n",
        "        print(f\"‚úÖ Train split: {len(train_split)} rows, Val split: {len(val_split)} rows\")\n",
        "        return train_split, val_split\n",
        "\n",
        "    def create_time_split(self, train_df):\n",
        "        \"\"\"Create time-based train-validation split using id5\"\"\"\n",
        "        print(\"üîÑ Creating time-based train-validation split...\")\n",
        "\n",
        "        # Convert id5 back to datetime for splitting\n",
        "        train_df['id5_dt'] = pd.to_datetime(train_df['id5'], format='%m-%d-%Y', errors='coerce')\n",
        "\n",
        "        # Check if conversion was successful\n",
        "        if train_df['id5_dt'].isna().all():\n",
        "            print(\"‚ö†Ô∏è Warning: All dates failed to convert. Trying different format...\")\n",
        "            train_df['id5_dt'] = pd.to_datetime(train_df['id5'], errors='coerce')\n",
        "\n",
        "        # Remove rows with invalid dates\n",
        "        valid_dates = ~train_df['id5_dt'].isna()\n",
        "        if valid_dates.sum() == 0:\n",
        "            raise ValueError(\"‚ùå No valid dates found in id5 column\")\n",
        "\n",
        "        train_df = train_df[valid_dates].copy()\n",
        "\n",
        "        # Use 80% earliest dates for training, 20% latest for validation\n",
        "        split_date = train_df['id5_dt'].quantile(0.8)\n",
        "        print(f\"üìÖ Split date: {split_date}\")\n",
        "\n",
        "        # Call the split function with the calculated cutoff\n",
        "        train_split, val_split = self._split_train_val_by_time(train_df, date_col='id5_dt', cutoff_date=split_date)\n",
        "\n",
        "        return train_split, val_split\n",
        "\n",
        "    def optimize_lightgbm_params(self, train_split, val_split):\n",
        "        \"\"\"Optimize LightGBM parameters for binary classification using Optuna\"\"\"\n",
        "        print(\"üîÑ Optimizing LightGBM parameters for classification...\")\n",
        "\n",
        "        def objective(trial):\n",
        "            # Suggest hyperparameters for binary classification\n",
        "            params = {\n",
        "                'objective': 'binary',\n",
        "                'boosting_type': 'gbdt',\n",
        "                'metric': 'auc',\n",
        "                'num_leaves': trial.suggest_int('num_leaves', 31, 200),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
        "                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "                'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
        "                'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "                'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "                'verbose': -1,\n",
        "                'seed': 42,\n",
        "                'n_jobs': -1\n",
        "            }\n",
        "            # In optimize_lightgbm_params()\n",
        "            params['feature_fraction'] = trial.suggest_float('feature_fraction', 0.4, 1.0)\n",
        "            params['top_rate'] = trial.suggest_float('top_rate', 0.1, 0.5)  # GOSS sampling\n",
        "            params['other_rate'] = trial.suggest_float('other_rate', 0.1, 0.5)\n",
        "\n",
        "            try:\n",
        "                X_train = train_split[self.feature_cols].fillna(0)\n",
        "                y_train = train_split['y']\n",
        "                X_val = val_split[self.feature_cols].fillna(0)\n",
        "                y_val = val_split['y']\n",
        "\n",
        "                train_dataset = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
        "                val_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset, free_raw_data=False)\n",
        "\n",
        "                model = lgb.train(\n",
        "                    params,\n",
        "                    train_dataset,\n",
        "                    num_boost_round=1000,\n",
        "                    valid_sets=[val_dataset],\n",
        "                    valid_names=['valid'],\n",
        "                    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
        "                )\n",
        "\n",
        "                return model.best_score['valid']['auc']\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Trial failed with error: {e}\")\n",
        "                return 0.0\n",
        "\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(objective, n_trials=10)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        best_params.update({\n",
        "            'objective': 'binary',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'metric': 'auc',\n",
        "            'verbose': -1\n",
        "        })\n",
        "\n",
        "        print(f\"‚úÖ Best LightGBM parameters: {best_params}\")\n",
        "        return best_params\n",
        "\n",
        "    def train_lightgbm_classifier(self, train_df, sampling_method='none'):\n",
        "        \"\"\"Train LightGBM binary classification model with sampling\"\"\"\n",
        "        print(\"üöÄ Training LightGBM classification ensemble...\")\n",
        "\n",
        "        self.models['lightgbm_ensemble'] = []\n",
        "\n",
        "        # Create time-based split for parameter tuning\n",
        "        train_split, val_split = self.create_time_split(train_df)\n",
        "\n",
        "        # Optimize parameters for classification\n",
        "        best_params = self.optimize_lightgbm_params(train_split, val_split)\n",
        "\n",
        "        # Prepare full training data\n",
        "        X_train = train_split[self.feature_cols].fillna(0)\n",
        "        y_train = train_split['y']\n",
        "\n",
        "        # Apply sampling\n",
        "        X_train, y_train = self.apply_sampling(X_train, y_train, sampling_method=sampling_method)\n",
        "\n",
        "        # Train an ensemble of models with different seeds for stability\n",
        "        for i in range(3):  # Reduced from 5 to 3 for efficiency\n",
        "            print(f\"üîÑ Training LightGBM model {i+1}/3...\")\n",
        "\n",
        "            model_params = best_params.copy()\n",
        "            model_params['seed'] = 42 + i\n",
        "\n",
        "            try:\n",
        "                train_dataset = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
        "\n",
        "                model = lgb.train(\n",
        "                    model_params,\n",
        "                    train_dataset,\n",
        "                    num_boost_round=1800,\n",
        "                    valid_sets=[train_dataset],\n",
        "                    callbacks=[\n",
        "                        lgb.early_stopping(150, verbose=False),\n",
        "                        lgb.log_evaluation(period=200)\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                self.models['lightgbm_ensemble'].append(model)\n",
        "                print(f\"‚úÖ Model {i+1} trained successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to train model {i+1}: {e}\")\n",
        "\n",
        "        print(f\"‚úÖ LightGBM ensemble trained with {len(self.models['lightgbm_ensemble'])} models\")\n",
        "\n",
        "    def optimize_xgboost_params(self, train_split, val_split):\n",
        "        \"\"\"Optimize XGBoost parameters using Optuna\"\"\"\n",
        "        print(\"üîÑ Optimizing XGBoost parameters...\")\n",
        "\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "\n",
        "            def objective(trial):\n",
        "                params = {\n",
        "                    'objective': 'rank:pairwise',  # Changed from rank:map\n",
        "                    'eval_metric': 'ndcg@7',      # Changed from map@7\n",
        "                    'eta': trial.suggest_float('eta', 0.01, 0.1),\n",
        "                    'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "                    'min_child_weight': trial.suggest_int('min_child_weight', 20, 100),\n",
        "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "                    'tree_method': 'hist',\n",
        "                    'seed': 42\n",
        "                }\n",
        "\n",
        "                # Prepare training data\n",
        "                X_train = train_split[self.feature_cols].fillna(0)\n",
        "                y_train = train_split['y']\n",
        "                groups_train = train_split['id2'].astype(int)\n",
        "                group_sizes_train = groups_train.value_counts().sort_index().values\n",
        "\n",
        "                # Prepare validation data\n",
        "                X_val = val_split[self.feature_cols].fillna(0)\n",
        "                y_val = val_split['y']\n",
        "                groups_val = val_split['id2'].astype(int)\n",
        "                group_sizes_val = groups_val.value_counts().sort_index().values\n",
        "\n",
        "                # Create DMatrix\n",
        "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "                dtrain.set_group(group_sizes_train)\n",
        "\n",
        "                dval = xgb.DMatrix(X_val, label=y_val)\n",
        "                dval.set_group(group_sizes_val)\n",
        "\n",
        "                # Train model\n",
        "                model = xgb.train(\n",
        "                    params,\n",
        "                    dtrain,\n",
        "                    num_boost_round=200,\n",
        "                    evals=[(dval, 'val')],\n",
        "                    early_stopping_rounds=30,\n",
        "                    verbose_eval=0\n",
        "                )\n",
        "\n",
        "                # Get validation score\n",
        "                val_score = model.best_score\n",
        "                return val_score\n",
        "\n",
        "            # Run optimization\n",
        "            study = optuna.create_study(direction='maximize')\n",
        "            study.optimize(objective, n_trials=15)\n",
        "\n",
        "            best_params = study.best_params\n",
        "            best_params.update({\n",
        "                'objective': 'rank:pairwise',\n",
        "                'eval_metric': 'ndcg@7',\n",
        "                'tree_method': 'hist'\n",
        "            })\n",
        "\n",
        "            print(f\"‚úÖ XGBoost Best parameters: {best_params}\")\n",
        "            print(f\"‚úÖ XGBoost Best score: {study.best_value}\")\n",
        "\n",
        "            return best_params\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è XGBoost optimization failed: {str(e)}\")\n",
        "            # Return default params\n",
        "            return {\n",
        "                'objective': 'rank:pairwise',\n",
        "                'eval_metric': 'ndcg@7',\n",
        "                'eta': 0.05,\n",
        "                'gamma': 1.0,\n",
        "                'max_depth': 8,\n",
        "                'min_child_weight': 50,\n",
        "                'subsample': 0.8,\n",
        "                'colsample_bytree': 0.8,\n",
        "                'tree_method': 'hist',\n",
        "                'seed': 42\n",
        "            }\n",
        "\n",
        "    def train_xgboost_classifier(self, train_df):\n",
        "        \"\"\"Train XGBoost binary classification model\"\"\"\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "            print(\"üöÄ Training XGBoost Classifier...\")\n",
        "\n",
        "            # For simplicity, we'll use fixed, robust parameters for XGBoost\n",
        "            # You can build an optimization function similar to LightGBM's if needed\n",
        "            params = {\n",
        "                'objective': 'binary:logistic',\n",
        "                'eval_metric': 'auc',\n",
        "                'eta': 0.05,\n",
        "                'max_depth': 7,\n",
        "                'subsample': 0.8,\n",
        "                'colsample_bytree': 0.8,\n",
        "                'min_child_weight': 50,\n",
        "                'tree_method': 'hist',\n",
        "                'seed': 42\n",
        "            }\n",
        "\n",
        "            X_train = train_df[self.feature_cols].fillna(0)\n",
        "            y_train = train_df['y']\n",
        "\n",
        "            # Create DMatrix (no groups needed for classification)\n",
        "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "\n",
        "            model = xgb.train(\n",
        "                params,\n",
        "                dtrain,\n",
        "                num_boost_round=1800,\n",
        "                evals=[(dtrain, 'train')],\n",
        "                early_stopping_rounds=150,\n",
        "                verbose_eval=100\n",
        "            )\n",
        "\n",
        "            self.models['xgboost'] = model\n",
        "            print(\"‚úÖ XGBoost training completed\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è XGBoost not installed. Skipping XGBoost training.\")\n",
        "            self.models['xgboost'] = None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è XGBoost training failed: {str(e)}\")\n",
        "            self.models['xgboost'] = None\n",
        "\n",
        "    def generate_r3_submission(self, test_data_path=\"/kaggle/input/test-data/test_data_r3.parquet\"):\n",
        "        \"\"\"\n",
        "        Special method to generate submission for round 3\n",
        "        Args:\n",
        "            test_data_path: Path to the R3 test data parquet file\n",
        "        \"\"\"\n",
        "        print(\"üöÄ Generating submission for Round 3\")\n",
        "\n",
        "        try:\n",
        "            # Load R3 test data\n",
        "            print(f\"üîÑ Loading test data from {test_data_path}\")\n",
        "            test_df = pd.read_parquet(test_data_path)\n",
        "\n",
        "            # Prepare features (use is_training=False)\n",
        "            print(\"üîß Preparing features...\")\n",
        "            test_df = self.prepare_features(test_df, is_training=False)\n",
        "\n",
        "            # Generate submission\n",
        "            submission = self.generate_submission(test_df, \"submission_r3.csv\")\n",
        "\n",
        "            return submission\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå R3 submission failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def generate_submission(self, test_df, output_file=\"submission_r3.csv\"):\n",
        "        \"\"\"\n",
        "        Generate submission file with all required columns filled\n",
        "        Args:\n",
        "            test_df: DataFrame from test_data_r3.parquet\n",
        "            output_file: Name of submission file to generate\n",
        "        \"\"\"\n",
        "        print(f\"üì§ Generating submission file: {output_file}\")\n",
        "\n",
        "        try:\n",
        "            # --- FIX: Ensure test features match training features ---\n",
        "            # 1. Get the columns the model was trained on\n",
        "            trained_features = self.feature_cols\n",
        "\n",
        "            # 2. Get the columns currently in the test dataframe\n",
        "            test_features = test_df.columns.tolist()\n",
        "\n",
        "            # 3. Identify missing columns\n",
        "            missing_cols = set(trained_features) - set(test_features)\n",
        "            if missing_cols:\n",
        "                print(f\"‚ö†Ô∏è Found {len(missing_cols)} missing columns in test data: {list(missing_cols)}\")\n",
        "                # Add missing columns and fill with a neutral value like 0\n",
        "                for c in missing_cols:\n",
        "                    test_df[c] = 0.5\n",
        "\n",
        "            # Get predictions from all models\n",
        "            predictions = []\n",
        "\n",
        "            # LightGBM ensemble predictions\n",
        "            if 'lightgbm_ensemble' in self.models and self.models['lightgbm_ensemble']:\n",
        "                print(\"üîÑ Predicting with LightGBM ensemble...\")\n",
        "                X_test = test_df[self.feature_cols].fillna(0)\n",
        "                lgb_preds = []\n",
        "                for model in self.models['lightgbm_ensemble']:\n",
        "                    try:\n",
        "                        pred = model.predict(X_test)\n",
        "                        lgb_preds.append(pred)\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è LightGBM prediction error: {str(e)}\")\n",
        "                if lgb_preds:\n",
        "                    predictions.append(np.mean(lgb_preds, axis=0))\n",
        "\n",
        "            # XGBoost predictions if available\n",
        "            if 'xgboost' in self.models and self.models['xgboost']:\n",
        "                print(\"üîÑ Predicting with XGBoost...\")\n",
        "                try:\n",
        "                    import xgboost as xgb\n",
        "                    dtest = xgb.DMatrix(test_df[self.feature_cols].fillna(0))\n",
        "                    xgb_pred = self.models['xgboost'].predict(dtest)\n",
        "                    predictions.append(xgb_pred)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è XGBoost prediction error: {str(e)}\")\n",
        "\n",
        "            if not predictions:\n",
        "                raise RuntimeError(\"No valid predictions were generated\")\n",
        "\n",
        "            # Ensemble predictions (simple average)\n",
        "            final_pred = np.mean(predictions, axis=0)\n",
        "\n",
        "            # Create submission dataframe with all required columns\n",
        "            submission = pd.DataFrame({\n",
        "                'id1': test_df['id1'],\n",
        "                'id2': test_df['id2'],\n",
        "                'id3': test_df['id3'],\n",
        "                'id5': test_df['id5'],\n",
        "                'pred': final_pred.astype(np.float32)\n",
        "            })\n",
        "\n",
        "            # Format datetime columns consistently\n",
        "\n",
        "            if pd.api.types.is_datetime64_any_dtype(submission['id5']):\n",
        "                submission['id5'] = submission['id5'].dt.strftime('%d-%m-%Y')\n",
        "\n",
        "            # Save to CSV\n",
        "            submission.to_csv(output_file, index=False)\n",
        "            print(f\"‚úÖ Submission saved to {output_file} with {len(submission)} rows\")\n",
        "            print(f\"üìä Prediction stats - Mean: {final_pred.mean():.4f}, Std: {final_pred.std():.4f}\")\n",
        "\n",
        "            return submission\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to generate submission: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def run_pipeline(self, generate_r3_submission=False, sampling_method='none'):\n",
        "        \"\"\"Complete pipeline execution with sampling option\"\"\"\n",
        "        print(f\"üöÄ Starting AMEX Click Prediction Pipeline with {sampling_method} sampling...\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load data\n",
        "            self.load_data()\n",
        "\n",
        "            # 2. Prepare training features\n",
        "            print(\"üîß Preparing training features...\")\n",
        "            train_df = self.prepare_features(self.train_data, is_training=True)\n",
        "\n",
        "            # 3. Train models with sampling\n",
        "            print(\"üéØ Training models...\")\n",
        "            self.train_lightgbm_classifier(train_df, sampling_method=sampling_method)\n",
        "            self.train_xgboost_classifier(train_df)\n",
        "\n",
        "            # 4. Feature importance analysis\n",
        "            self.display_feature_importance(train_df)\n",
        "\n",
        "            # 5. Generate submission\n",
        "            if generate_r3_submission:\n",
        "                self.generate_r3_submission()\n",
        "            else:\n",
        "                print(\"üîß Preparing test features...\")\n",
        "                test_df = self.prepare_features(self.test_data, is_training=False)\n",
        "                self.generate_submission(test_df)\n",
        "\n",
        "            print(\"‚úÖ Pipeline completed successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Pipeline failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Execute pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = AMEXClickPredictionPipeline()\n",
        "    pipeline.run_pipeline(generate_r3_submission=True, sampling_method='smote')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook9b32cb8c3a",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 7824410,
          "isSourceIdPinned": false,
          "sourceId": 12407101,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
